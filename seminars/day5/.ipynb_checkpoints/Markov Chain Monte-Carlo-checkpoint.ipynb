{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RhDXMU1_qceo"
   },
   "outputs": [],
   "source": [
    "!pip install numpy torch statsmodels matplotlib seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XqWsGWXQqZOH"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.distributions as dist\n",
    "\n",
    "from statsmodels.tsa.stattools import acf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wTfJDykqqZOJ"
   },
   "source": [
    "# MCMC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XtdTlQQYqZOJ"
   },
   "source": [
    "## Metropolis-Hastings rejection\n",
    "\n",
    "Let $\\pi(\\theta)$ be the distribution from which we want to sample. Suppose we are given unnormalized density $\\hat{\\pi}(\\theta) = \\frac{\\pi(\\theta)}{Z}$ where $Z$ is the normalizing constant. In MCMC methods we choose proposal density $q(\\theta' \\mid \\theta)$, initial point $\\theta_0$ and then we build a Markov chain like that:\n",
    "\n",
    "For $t = 0, 1,\\dots$: \n",
    "1. sample $\\hat{\\theta}_{t+1} \\sim q(\\theta' \\mid \\theta_t)$\n",
    "2. Apply Metropolis-Hastings rejection:\n",
    "$$\\theta_{t+1} = \\begin{cases}\n",
    "\\hat{\\theta}_{t+1}, \\text{ with probabilty } \\rho(\\hat{\\theta}_{t+1} \\mid \\theta_t) \\\\\n",
    "\\theta_t, \\text{ with probabilty } 1 - \\rho(\\hat{\\theta}_{t+1} \\mid \\theta_t)\n",
    "\\end{cases}$$\n",
    "where \n",
    "$$\\rho(\\theta' \\mid \\theta) = \\min \\left(1, \\frac{\\hat{\\pi}(\\theta')q(\\theta \\mid \\theta')}{\\hat{\\pi}(\\theta)q(\\theta' \\mid \\theta)} \\right)$$\n",
    "\n",
    "Below you can see Python class for MCMC method. You need to implement `acceptance_prob` method which computes $\\rho(\\theta' \\mid \\theta)$. Everything else is already implemented for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YKUggv-7qZOK"
   },
   "outputs": [],
   "source": [
    "class Distribution:\n",
    "    \"\"\"Abstract class for unnormalized distribution\"\"\"\n",
    "    \n",
    "    def log_density(self, x):\n",
    "        \"\"\"\n",
    "            Computes vectorized log of unnormalized log density\n",
    "            \n",
    "            x (torch tensor of shape BxD): B points at which we compute log density\n",
    "            returns (torch tensor of shape B): \\log \\hat{\\pi}(x) \n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def grad_log_density(self, x):\n",
    "        \"\"\"\n",
    "            Computes vectorized gradient \\nabla_x \\log \\pi(x)\n",
    "            \n",
    "            x (torch tensor of shape BxD): point at which we compute \\nabla \\log \\pi\n",
    "            returns (torch.tensor of shape BxD): gradients of log density\n",
    "        \"\"\"\n",
    "        x = x.clone().requires_grad_()\n",
    "        logp = self.log_density(x)\n",
    "        logp.sum().backward()\n",
    "        return x.grad\n",
    "\n",
    "class Proposal:\n",
    "    \"\"\"Abstract class for proposal\"\"\"\n",
    "    \n",
    "    def sample(self, x):\n",
    "        \"\"\"\n",
    "            Computes vectorized sample from proposal q(x' | x)\n",
    "            \n",
    "            x (torch tensor of shape BxD): current point from which we propose\n",
    "            returns: (torch tensor of shape BxD) new points\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def log_density(self, x, x_prime):\n",
    "        \"\"\"\n",
    "            Computes vectorized log of unnormalized log density\n",
    "            \n",
    "            x (torch tensor of shape BxD): B points at which we compute log density\n",
    "            returns (torch tensor of shape B): \\log q(x' | x) \n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "        \n",
    "class MCMC:\n",
    "    def __init__(self, distribution, proposal):\n",
    "        \"\"\"\n",
    "            Constructs MCMC sampler\n",
    "        \n",
    "            distribution (Distribution): distribution from which we sample\n",
    "            proposal (Proposal): MCMC proposal\n",
    "        \"\"\"\n",
    "        self.distribution = distribution\n",
    "        self.proposal = proposal\n",
    "    \n",
    "    def _step(self, x):\n",
    "        x_prime = self.proposal.sample(x)\n",
    "        acceptance_prob = self.acceptance_prob(x_prime, x)\n",
    "        \n",
    "        mask = torch.rand(x.shape[0]) < acceptance_prob\n",
    "        x[mask] = x_prime[mask]\n",
    "        self._rejected += (1 - mask).type(torch.float32)\n",
    "        return x\n",
    "\n",
    "    def simulate(self, initial_point, n_steps, n_parallel=10):\n",
    "        \"\"\"\n",
    "            Run `n_parallel ` simulations for `n_steps` starting from `initial_point`\n",
    "            \n",
    "            initial_point (torch tensor of shape D): starting point for all chains\n",
    "            n_steps (int): number of samples in Markov chain\n",
    "            n_parallel (int): number of parallel chains\n",
    "            returns: dict(\n",
    "                points (torch tensor of shape n_parallel x n_steps x D): samples\n",
    "                n_rejected (numpy array of shape n_parallel): number of rejections for each chain\n",
    "                rejection_rate (float): mean rejection rate over all chains\n",
    "                means (torch tensor of shape n_parallel x n_steps x D): means[c, s] = mean(points[c, :s])\n",
    "                variances (torch tensor of shape n_parallel x n_steps x D): variances[c, s, d] = variance(points[c, :s, d])\n",
    "            )\n",
    "        \"\"\"\n",
    "        xs = []\n",
    "        x = initial_point.repeat(n_parallel, 1)\n",
    "        self._rejected = torch.zeros(n_parallel)\n",
    "        \n",
    "        dim = initial_point.shape[0]\n",
    "        sums = np.zeros([n_parallel, dim])\n",
    "        squares_sum = np.zeros([n_parallel, dim])\n",
    "        \n",
    "        means = []\n",
    "        variances = []        \n",
    "        \n",
    "        for i in range(n_steps):\n",
    "            x = self._step(x)\n",
    "            xs.append(x.numpy().copy())\n",
    "            \n",
    "            sums += xs[-1]\n",
    "            squares_sum += xs[-1]**2\n",
    "            \n",
    "            mean, squares_mean = sums / (i + 1), squares_sum / (i + 1)\n",
    "            means.append(mean.copy())\n",
    "            variances.append(squares_mean - mean**2)\n",
    "        \n",
    "        xs = np.stack(xs, axis=1)        \n",
    "        means = np.stack(means, axis=1)\n",
    "        variances = np.stack(variances, axis=1)\n",
    "        \n",
    "        return dict(\n",
    "            points=xs,\n",
    "            n_rejected=self._rejected.numpy(),\n",
    "            rejection_rate=(self._rejected / n_steps).mean().item(),\n",
    "            means=means,\n",
    "            variances=variances\n",
    "        )\n",
    "        \n",
    "    def acceptance_prob(self, x_prime, x):\n",
    "        \"\"\"\n",
    "            In this function you need to compute \n",
    "            probability of acceptance \\rho(x' | x)\n",
    "\n",
    "            x_prime (numpy array): new point\n",
    "            x (numpy array): current point\n",
    "            returns: acceptance probability \\rho(x', x)\n",
    "        \"\"\"\n",
    "    \n",
    "        # TODO\n",
    "        pass\n",
    "    \n",
    "def simulate(distribution, proposal, initial_point, n_samples, n_parallel=10):\n",
    "    mcmc = MCMC(distribution, proposal)\n",
    "    return mcmc.simulate(initial_point, n_samples, n_parallel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ki1naAEx1JnC"
   },
   "source": [
    "# Plotting utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s0M1xFjKqZOL"
   },
   "outputs": [],
   "source": [
    "def plot_points(xs, ax, i=0, j=1, color=True):\n",
    "    ax.set_title('points')\n",
    "    ax.set_xlabel(f'coordinate {i}')\n",
    "    ax.set_ylabel(f'coordinate {j}')\n",
    "    \n",
    "    n_parallel, n_samples, _ = xs.shape\n",
    "    c = np.arange(n_samples) if color else None\n",
    "    for k in range(n_parallel):\n",
    "        ax.scatter(xs[k, :, i], xs[k, :, j], s=5, c=c)\n",
    "    return ax\n",
    "\n",
    "def plot_log_density(xs, ax, distribution):\n",
    "    ax.set_title('log_density')\n",
    "    ax.set_xlabel('iteration')\n",
    "    ax.set_ylabel('log density')\n",
    "    \n",
    "    n_parallel, n_samples, _ = xs.shape\n",
    "    for k in range(n_parallel):\n",
    "        density = distribution.log_density(torch.tensor(xs[k]))\n",
    "        ax.plot(density.numpy(), label=f'run {k + 1}')\n",
    "    ax.legend(loc='best')\n",
    "    return ax\n",
    "\n",
    "def find_first(X):\n",
    "    for i in range(X.shape[0]):\n",
    "        if X[i]:\n",
    "            return i\n",
    "    return -1\n",
    "  \n",
    "def integrated_autocorr(x, acf_cutoff=0.0):\n",
    "    if x.ndim == 1:\n",
    "        x = x.reshape(-1, 1)\n",
    "    n = len(x)\n",
    "\n",
    "    tau = np.zeros(x.shape[1])\n",
    "    for j in range(x.shape[1]):\n",
    "        f = acf(x[:,j], nlags=n, unbiased=False, fft=True)\n",
    "        window = find_first((f <= acf_cutoff).astype(np.uint8))\n",
    "        tau[j] = 1 + 2*f[1:window].sum()\n",
    "\n",
    "    return tau\n",
    "\n",
    "def compute_ess(x, acf_cutoff=0.0):\n",
    "    tau = integrated_autocorr(x, acf_cutoff=acf_cutoff)\n",
    "    return x.shape[0] / tau\n",
    "  \n",
    "def plot_autocorr(xs, axes, step=100, mean=False, label=None):\n",
    "    n_parallel, _, dim = xs.shape\n",
    "    \n",
    "    for i in range(dim):\n",
    "        axes[i].set_title(f'autocorrelation (step={step}, coordinate {i})')\n",
    "        axes[i].set_xlabel('lag')\n",
    "        axes[i].set_ylabel('autocorrelation')\n",
    "        \n",
    "        acfs = [acf(xs[k, ::step, i]) for k in range(n_parallel)]\n",
    "        ess = np.stack([compute_ess(xs[k]) for k in range(n_parallel)], axis=0)\n",
    "        \n",
    "        if mean:\n",
    "            axes[i].plot(np.mean(acfs, axis=0), marker='o', label=(label or '') + f\" ESS = {ess.mean(axis=0)[i]:.2f}\")\n",
    "        else:\n",
    "            for k, acf_ in enumerate(acfs):\n",
    "                axes[i].plot(acf_, marker='o', label=f'run {k} ESS = {ess[k][i]:.2f}')\n",
    "            \n",
    "        axes[i].legend(loc='best')\n",
    "    return axes\n",
    "        \n",
    "def cummean(arr, axis=0):\n",
    "    if axis < 0:\n",
    "        axis = axis + len(arr.shape)\n",
    "    \n",
    "    arange = np.arange(1, arr.shape[axis] + 1)\n",
    "    arange = arange.reshape((1,) * axis + (-1,) + (1,) * (len(arr.shape) - axis - 1))\n",
    "    return arr.cumsum(axis=axis) / arange\n",
    "    \n",
    "def plot_statistics(xs, axes, skip=0, step=1):\n",
    "    xs = xs[:, skip::step]\n",
    "    n_parallel, _, dim = xs.shape\n",
    "    \n",
    "    means = cummean(xs, axis=1)\n",
    "    variances = cummean(xs**2, axis=1) - means**2\n",
    "    \n",
    "    for i in range(n_parallel):\n",
    "        for j in range(dim):\n",
    "            ax = axes[j]\n",
    "            ax.set_title(f'coordinate {j} running mean and std')\n",
    "            ax.set_xlabel('iteration')\n",
    "            ax.set_ylabel(f'coordinate {j}')\n",
    "            \n",
    "            x = np.arange(means.shape[1])\n",
    "            y = means[i, :, j]\n",
    "            e = np.sqrt(variances[i, :, j])\n",
    "            \n",
    "            r = ax.plot(y, label=f'chain {i} mean')\n",
    "            ax.plot(e, linestyle='--', c=r[0].get_color(), label=f'chain {i} std')\n",
    "            ax.legend(loc='best')\n",
    "        \n",
    "    return axes\n",
    "  \n",
    "def plot_traceplot(xs, axes):\n",
    "    n_parallel, _, dim = xs.shape\n",
    "    \n",
    "    for i in range(n_parallel):\n",
    "        for j in range(dim):\n",
    "            ax = axes[j]\n",
    "            ax.set_title(f'coordinate {j} traceplot')\n",
    "            ax.set_xlabel('iteration')\n",
    "            ax.set_ylabel(f'coordinate {j}')\n",
    "           \n",
    "            ax.plot(xs[i, :, j], label=f'chain {i}')\n",
    "            ax.legend(loc='best')\n",
    "        \n",
    "    return axes\n",
    "\n",
    "def plot_distribution(distribution, bounds, ax, num=50, n_levels=None, filled=False, exp=False):\n",
    "    x_min, x_max = bounds[0]\n",
    "    y_min, y_max = bounds[1]\n",
    "    \n",
    "    ax.set_xlim(x_min, x_max)\n",
    "    ax.set_ylim(y_min, y_max)\n",
    "    x, y = np.meshgrid(np.linspace(x_min, x_max, num=num), np.linspace(y_min, y_max, num=num))\n",
    "    s = x.shape\n",
    "    xy = np.stack([x.reshape(-1), y.reshape(-1)], axis=1)\n",
    "    z = distribution.log_density(torch.tensor(xy, dtype=torch.float32)).numpy().reshape(s)\n",
    "    if exp:\n",
    "      z = np.exp(z)\n",
    "    \n",
    "    plot = ax.contourf if filled else ax.contour\n",
    "    r = plot(x, y, z, n_levels)\n",
    "    return ax, r\n",
    "\n",
    "def plot_distribution_grad(distribution, bounds, ax, num=50):\n",
    "    x_min, x_max = bounds[0]\n",
    "    y_min, y_max = bounds[1]\n",
    "    \n",
    "    ax.set_xlim(x_min, x_max)\n",
    "    ax.set_ylim(y_min, y_max)\n",
    "    x, y = np.meshgrid(np.linspace(x_min, x_max, num=num), np.linspace(y_min, y_max, num=num))\n",
    "   \n",
    "    s = x.shape\n",
    "    xy = np.stack([x.reshape(-1), y.reshape(-1)], axis=1)\n",
    "    z = distribution.grad_log_density(torch.tensor(xy, dtype=torch.float32)).numpy()\n",
    "    u, v = z[..., 0], z[..., 1]\n",
    "    c = np.sqrt(u**2 + v**2)\n",
    "    \n",
    "    ax.quiver(x, y, u, v, c, angles='xy')\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "heading_collapsed": true,
    "id": "r8Hm6eohqZON"
   },
   "source": [
    "# Distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "hidden": true,
    "id": "0meZsrCMqZON"
   },
   "source": [
    "In this seminar we consider four distributions on which we test different properties of MCMC methods.\n",
    "1. Typical normal isotropic distribution: $$\\pi(x) = \\mathcal{N}(x \\mid \\mu, \\sigma^2 I)$$\n",
    "2. Scaled normal distribution: $$\\pi(x) = \\mathcal{N}(x \\mid \\mu, D)$$\n",
    "3. Mixture of normals: $$\\pi(x) = \\frac{1}{N}\\sum_{k=1}^N \\mathcal{N}(x \\mid \\mu_k, D_k)$$\n",
    "4. Langevin mixture: $$\n",
    "\\begin{align*}\n",
    "    & \\pi(\\theta) = p(\\theta \\mid X) \\propto p(X \\mid \\theta)p(\\theta) \\\\\n",
    "    & p(X \\mid \\theta) = \\prod_{k=1}^N \\left[ \\frac{1}{2}\\mathcal{N}(x_k \\mid \\theta_1, \\sigma_x^2) + \\frac{1}{2}\\mathcal{N}(x_k \\mid \\theta_1 + \\theta_2, \\sigma_x^2) \\right] \\\\\n",
    "    & p(\\theta) = \\mathcal{N}(\\theta_1 \\mid 0, \\sigma_1^2)\\mathcal{N}(\\theta_2 \\mid 0, \\sigma_2^2)\n",
    "\\end{align*}\n",
    "$$ This is example is from [the paper](https://www.ics.uci.edu/~welling/publications/papers/stoclangevin_v6.pdf) on Langevin Dynamics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uayRP1C0h0KD"
   },
   "source": [
    "**Hint:** mind the shape, `log_density` assumes that input is a batch of points (for running several parallel MCMC chains)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "X2GxyuWKqZOO"
   },
   "outputs": [],
   "source": [
    "class Normal(Distribution):\n",
    "    \"\"\"Represents normal distribution N(mean, std)\"\"\"\n",
    "    \n",
    "    def __init__(self, loc, scale):\n",
    "        \"\"\"\n",
    "            loc (torch tensor of shape D): mean\n",
    "            scale (torch tensor of shape D) std\n",
    "        \"\"\"\n",
    "        self.dist = dist.Normal(loc=loc, scale=scale)\n",
    "\n",
    "    def log_density(self, x):\n",
    "        # TODO\n",
    "        pass\n",
    "    \n",
    "\n",
    "class MixtureOfNormals(Distribution):\n",
    "    \"\"\"Represents mixture of normals: \\pi(x) = \\sum_k \\pi_k N(x | mean_k, std_k)\"\"\"\n",
    "    def __init__(self, locs, scales, pi):\n",
    "        \"\"\"\n",
    "            locs (torch tensor of shape NxD): locs[k] = mean_k\n",
    "            scales (torch tensor of shape NxD): scales[k] = std_k\n",
    "            pi (torch.tensor of shape N): pi[k] = pi_k\n",
    "        \"\"\"\n",
    "        self.dists = [\n",
    "            dist.Normal(loc=loc, scale=scale)\n",
    "            for loc, scale in zip(locs, scales)\n",
    "        ]\n",
    "        self.pi = pi\n",
    "        \n",
    "    def log_density(self, x):\n",
    "        # TODO\n",
    "        # Hint: use logsumexp\n",
    "        pass\n",
    "\n",
    "    \n",
    "class LangevinMixture(Distribution):\n",
    "    \"\"\"\n",
    "        This distribution is defined as posterior on theta_1, theta_2 given\n",
    "        prior: theta_1 ~ N(0, \\sigma_1^2), theta_2 ~ N(0, \\sigma_2^2)\n",
    "        likelihood: x_i ~ 0.5 * N(theta_1, \\sigma_x^2) + 0.5 * N(theta_1 + theta_2, \\sigma_x^2)\n",
    "        \n",
    "        More detailed information can be found at \n",
    "        https://www.ics.uci.edu/~welling/publications/papers/stoclangevin_v6.pdf\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, theta1=0., theta2=1., \n",
    "                       s1=np.sqrt(10.), s2=1., sx=np.sqrt(2.), N=100):\n",
    "        mu = torch.tensor([theta1, theta1 + theta2])\n",
    "        pi = np.random.binomial(n=1, p=0.5, size=N).astype(np.int64)\n",
    "\n",
    "        d = dist.Normal(loc=mu, scale=sx)\n",
    "        self.X = d.sample((N,))[np.arange(N), pi]\n",
    "    \n",
    "        self.d1 = dist.Normal(loc=0., scale=s1)\n",
    "        self.d2 = dist.Normal(loc=0., scale=s2)\n",
    "        self.dx = dist.Normal(loc=0., scale=sx)\n",
    "    \n",
    "    def log_density(self, theta):\n",
    "        theta1, theta2 = theta[..., 0], theta[..., 1]\n",
    "        log_prior = self.d1.log_prob(theta1) + self.d2.log_prob(theta2)\n",
    "        \n",
    "        mu = torch.stack([theta1, theta1 + theta2], dim=-1)\n",
    "        \n",
    "        log_likelihood = self.dx.log_prob(self.X[None, :, None] - mu[:, None])\n",
    "        log_likelihood = torch.logsumexp(log_likelihood, dim=-1)\n",
    "        return log_prior + log_likelihood.sum(dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "heading_collapsed": true,
    "id": "AzwBR-nvqZOP"
   },
   "source": [
    "# Proposals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "hidden": true,
    "id": "4zCoYsD2qZOQ"
   },
   "source": [
    "We will study three proposal distributions.\n",
    "\n",
    "1. Random walk:\n",
    "$$q(\\theta' \\mid \\theta) = \\mathcal{N}(\\theta' \\mid \\theta, \\sigma^2)$$\n",
    "2. Langevin dynamics:\n",
    "$$q(\\theta' \\mid \\theta) = \\mathcal{N}\\left(\\theta' \\mid \\theta - \\frac{\\varepsilon}{2}\\nabla \\log \\pi(\\theta), \\varepsilon \\right)$$\n",
    "3. Hamiltonian Monte-Carlo (HMC):\n",
    "$$\n",
    "\\begin{align*}\n",
    "    & r \\sim \\mathcal{N}(0, I) \\\\\n",
    "    & \\theta', r' = \\text{integrate}(\\theta, r)\n",
    "\\end{align*}\n",
    "$$\n",
    "Integrate refers to Hamiltonian dynamics:\n",
    "$$\n",
    "\\begin{align*}\n",
    "    & H(\\theta, r) = -\\log \\pi(\\theta) + \\frac{1}{2}r^T r \\\\ \n",
    "    & \\frac{d\\theta}{dt} = \\frac{\\partial H}{\\partial r} \\\\\n",
    "    & \\frac{dr}{dt} = -\\frac{\\partial H}{\\partial \\theta}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "You need to implement all three proposals. For HMC you need to code different integration schemes. Here we take rather naive approach to HMC implementation. In practice you would use adaptive schemes like [NUTS](https://arxiv.org/abs/1111.4246)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "UYVrQ2h1qZOQ"
   },
   "outputs": [],
   "source": [
    "class RandomWalk(Proposal):\n",
    "    \"\"\"Proposal of the form q(x' | x) = N(x' | x, \\sigma^2)\"\"\"\n",
    "    \n",
    "    def __init__(self, sigma):\n",
    "        self.sigma = sigma\n",
    "        self.d = dist.Normal(loc=0., scale=sigma)\n",
    "        \n",
    "    def sample(self, x):\n",
    "        # TODO\n",
    "        pass\n",
    "    \n",
    "    def log_density(self, x, x_prime):\n",
    "        # TODO\n",
    "        pass\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f\"Random walk sigma={self.sigma}\"\n",
    "    \n",
    "class Langevin(Proposal):\n",
    "    \"\"\"Proposal given by q(x' | x) = N(x' | x - 0.5 * eps * \\nabla \\log \\pi(x), eps)\"\"\"\n",
    "    def __init__(self, eps, d):\n",
    "        self.d = dist.Normal(loc=0., scale=np.sqrt(eps))\n",
    "        self.dist = d\n",
    "        self.eps = eps\n",
    "        \n",
    "    def sample(self, x):\n",
    "        # TODO\n",
    "        pass        \n",
    "    \n",
    "    def log_density(self, x, x_prime):\n",
    "        # TODO\n",
    "        pass\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f\"Langevin eps={self.eps}\"\n",
    "\n",
    "class HMC(Proposal):\n",
    "    \"\"\"HMC proposal\"\"\"\n",
    "    \n",
    "    def __init__(self, eps, d, n_steps=5, method='leapfrog'):\n",
    "        self.d = dist.Normal(loc=0., scale=1.)\n",
    "        self.dist = d\n",
    "        self.eps = eps\n",
    "        self.n_steps = n_steps\n",
    "        self.method = method\n",
    "        self._method = {\n",
    "            'leapfrog': self._leapfrog,\n",
    "            'euler': self._euler,\n",
    "            'simple_euler': self._simple_euler\n",
    "        }[method]\n",
    "          \n",
    "    def _energy(self, x, v):\n",
    "        return -self.dist.log_density(x) + self.d.log_prob(v).sum(dim=-1)\n",
    "        \n",
    "    def _leapfrog(self, x, v):\n",
    "        # TODO\n",
    "        pass\n",
    "    \n",
    "    def _euler(self, x, v):\n",
    "        # TODO\n",
    "        pass\n",
    "    \n",
    "    def _simple_euler(self, x, v):\n",
    "        self.energy = []\n",
    "        for _ in range(self.n_steps):\n",
    "            x, v = x + self.eps * v, v - self.eps * self.dist.grad_log_density(x)\n",
    "            self.energy.append(self._energy(x, v))\n",
    "        return x, v\n",
    "        \n",
    "    def sample(self, x):\n",
    "        v = self.d.sample(sample_shape=x.shape)\n",
    "        self.v0 = v.clone()\n",
    "        self.x0 = x.clone()\n",
    "        \n",
    "        x, v = self._method(x, v)\n",
    "        self.v = v\n",
    "        return x\n",
    "    \n",
    "    def log_density(self, x, x_prime):\n",
    "        if torch.norm(x - self.x0).item() < 1e-5:\n",
    "            return self.d.log_prob(self.v0).sum(dim=-1)\n",
    "        else:\n",
    "            return self.d.log_prob(self.v).sum(dim=-1)\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f\"HMC eps={self.eps}, n_steps={self.n_steps}, method={self.method}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "40-sjk9jqZOR"
   },
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ejE7UjGwqZOS"
   },
   "source": [
    "First of all, let's plot all the probability distributions to better understand what we deal with. Plots are in log-domain, pass `exp=True` to `plot_distribution` to have the real density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_mkk8W1CqZOS"
   },
   "outputs": [],
   "source": [
    "standard_normal = Normal(loc=0., scale=1.)\n",
    "scaled_normal = Normal(loc=0., scale=torch.tensor([10., 1.]))\n",
    "mixture = MixtureOfNormals(\n",
    "    locs=[torch.tensor([0., 0.]), torch.tensor([4., 4.])], \n",
    "    scales=[1., 1.],\n",
    "    pi=torch.tensor([0.5, 0.5])\n",
    ")\n",
    "\n",
    "langevin = LangevinMixture()\n",
    "\n",
    "fig, axes = plt.subplots(ncols=2, nrows=2, figsize=(10, 10))\n",
    "plot_distribution(standard_normal, bounds=((-3, 3), (-3, 3)), ax=axes[0][0], num=100, n_levels=20)\n",
    "plot_distribution_grad(standard_normal, bounds=((-3, 3), (-3, 3)), ax=axes[0][0], num=20)\n",
    "axes[0][0].set_title('standard normal')\n",
    "\n",
    "plot_distribution(scaled_normal, bounds=((-100, 100), (-100, 100)), ax=axes[0][1], num=100, n_levels=20)\n",
    "plot_distribution_grad(scaled_normal, bounds=((-100, 100), (-100, 100)), ax=axes[0][1], num=20)\n",
    "axes[0][1].set_title('scaled normal')\n",
    "\n",
    "plot_distribution(mixture, bounds=((-4, 8), (-4, 8)), ax=axes[1][0], num=100, n_levels=20)\n",
    "plot_distribution_grad(mixture, bounds=((-4, 8), (-4, 8)), ax=axes[1][0], num=20)\n",
    "axes[1][0].set_title('mixture of normals')\n",
    "\n",
    "plot_distribution(langevin, bounds=((-3, 3), (-3, 3)), ax=axes[1][1], num=100, n_levels=20)\n",
    "plot_distribution_grad(langevin, bounds=((-3, 3), (-3, 3)), ax=axes[1][1], num=20)\n",
    "axes[1][1].set_title('langevin mixture');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "heading_collapsed": true,
    "id": "JmBZEkmeqZOV"
   },
   "source": [
    "## Sampler tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "hidden": true,
    "id": "pbqIDq-RqZOV"
   },
   "source": [
    "The most important issue when working with MCMC is the hyperparameter tuning. One useful quantity to track is rejection rate. Plot samples and rejection rate for different proposals and hyperparameters. What can you say about dependence of rejection rate and quality of sampling?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "33bdqmALqZOW"
   },
   "outputs": [],
   "source": [
    "def tuning_experiment(distribution, proposal, hyperparameters, options, color=True):\n",
    "    fig, axes = plt.subplots(ncols=len(hyperparameters), figsize=(20, 20))\n",
    "\n",
    "    for i, hp in enumerate(hyperparameters):\n",
    "        result = simulate(distribution, proposal(**hp), **options)\n",
    "        ax = axes[i]\n",
    "        plot_points(result['points'], ax, color=color)\n",
    "        \n",
    "        ax.set_aspect('equal')\n",
    "        hp_string = ', '.join(f'{k}={v}' for k, v in hp.items())\n",
    "        ax.set_title(f'{hp_string}, rejection rate = {result[\"rejection_rate\"]:.2f}')\n",
    "        ax.set_xlim(-4., 4.)\n",
    "        ax.set_ylim(-4., 4.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RVB4C4Qy8y9N"
   },
   "source": [
    "***Note:*** by default `plot_points` makes the color of point brighter if it is the point from later iteration. You can see different chains by setting `color=False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "s7Ca95nIqZOX"
   },
   "outputs": [],
   "source": [
    "# choose some sigmas and run simulations\n",
    "sigmas = ... # TODO\n",
    "\n",
    "tuning_experiment(\n",
    "    standard_normal,\n",
    "    proposal=RandomWalk,\n",
    "    hyperparameters=[dict(sigma=s) for s in sigmas],\n",
    "    options = dict(\n",
    "        initial_point=torch.tensor([0., 0.]),\n",
    "        n_samples=1000,\n",
    "        n_parallel=10\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "1pZfOiSnqZOZ"
   },
   "outputs": [],
   "source": [
    "# choose some step sizes and run simulation\n",
    "step_sizes = ... # TODO\n",
    "\n",
    "proposal = lambda eps: Langevin(eps, d=standard_normal)\n",
    "\n",
    "tuning_experiment(\n",
    "    standard_normal,\n",
    "    proposal=proposal,\n",
    "    hyperparameters=[dict(eps=e) for e in step_sizes],\n",
    "    options = dict(\n",
    "        initial_point=torch.tensor([0., 0.]),\n",
    "        n_samples=1000,\n",
    "        n_parallel=10\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "heading_collapsed": true,
    "id": "tEql8eBrqZOa"
   },
   "source": [
    "## Scale of distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "hidden": true,
    "id": "NAM0dH45qZOb"
   },
   "source": [
    "Use the same proposals but for the scaled normal distribution. How do hyperparameters and number of samples necessary to cover the distribution change?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "2OB-O9JnqZOb"
   },
   "outputs": [],
   "source": [
    "def run_many_proposals(distribution, proposals, options, bounds=[(-40, 40), (-4, 4)]):\n",
    "    fig, axes = plt.subplots(nrows=len(proposals), figsize=(20, 10))\n",
    "\n",
    "    for i, proposal in enumerate(proposals):\n",
    "        result = simulate(distribution, proposal, **options)\n",
    "        ax = axes[i]\n",
    "        plot_points(result['points'], ax)\n",
    "        ax.set_title(f'{proposal}, rejection rate = {result[\"rejection_rate\"]:.2f}')\n",
    "        ax.set_xlim(*bounds[0])\n",
    "        ax.set_ylim(*bounds[1])\n",
    "        ax.set_aspect('equal')\n",
    "\n",
    "    fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "3MNE3qQPqZOd"
   },
   "outputs": [],
   "source": [
    "# Try out different proposals\n",
    "proposals = ... # TODO\n",
    "\n",
    "run_many_proposals(\n",
    "    scaled_normal,\n",
    "    proposals=proposals,\n",
    "    options = dict(\n",
    "        initial_point=torch.tensor([0., 0.]),\n",
    "        n_samples=1000,\n",
    "        n_parallel=10\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "pxmkKGWTqZOe"
   },
   "outputs": [],
   "source": [
    "# Now with 10x more samples\n",
    "\n",
    "run_many_proposals(\n",
    "    scaled_normal,\n",
    "    proposals=proposals,\n",
    "    options = dict(\n",
    "        initial_point=torch.tensor([0., 0.]),\n",
    "        n_samples=10000,\n",
    "        n_parallel=10\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "heading_collapsed": true,
    "id": "7jL6j-UPqZOg"
   },
   "source": [
    "## Autocorrelation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "hidden": true,
    "id": "7r-CoFuHqZOg"
   },
   "source": [
    "Another aspect which you can use to tune your sampler is the autocorrelation of the samples. Plot autocorrelation for different proposals. Also try to take every 10th or 100th sample and watch how autocorrelation changes. A useful thing to measure when working with MCMC is the [effective sample size](https://en.wikipedia.org/wiki/Effective_sample_size) (ESS) which is defined as $$\\text{ESS} = \\frac{\\text{number of samples}}{\\text{autocorrelation time}}$$ \n",
    "\n",
    "In a nutshell, it says how many *almost uncorrelated* samples there are in chain. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "3_aw-AL4qZOg"
   },
   "outputs": [],
   "source": [
    "def autocorr_experiment(distribution, proposals, steps, options):\n",
    "    fig, axes = plt.subplots(nrows=len(steps), ncols=options['initial_point'].shape[0], figsize=(20, 10))\n",
    "\n",
    "    for proposal in proposals:\n",
    "        result = simulate(distribution, proposal, **options)\n",
    "        xs = result['points']\n",
    "        \n",
    "        for i, step in enumerate(steps):\n",
    "            plot_autocorr(xs, axes[i], step=step, mean=True, label=f'{proposal}')\n",
    "    \n",
    "    fig.tight_layout()\n",
    "    fig.subplots_adjust(hspace=0.5)\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "Gb_GR2h1qZOi"
   },
   "outputs": [],
   "source": [
    "proposals = ... # TODO\n",
    "\n",
    "autocorr_experiment(\n",
    "    distribution=standard_normal,\n",
    "    proposals=proposals,\n",
    "    steps=[1, 10, 100],\n",
    "    options = dict(\n",
    "        initial_point=torch.tensor([0., 0.]),\n",
    "        n_samples=1000,\n",
    "        n_parallel=10\n",
    "    )\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "u3S7e78yqZOj"
   },
   "outputs": [],
   "source": [
    "proposals = ... # TODO\n",
    "\n",
    "autocorr_experiment(\n",
    "    distribution=scaled_normal,\n",
    "    proposals=proposals,\n",
    "    steps=[1, 10, 100],\n",
    "    options=dict(\n",
    "        initial_point=torch.tensor([0., 0.]),\n",
    "        n_samples=1000,\n",
    "        n_parallel=10\n",
    "    )\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "heading_collapsed": true,
    "id": "t9mPos2nqZOl"
   },
   "source": [
    "## Multi-modality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "hidden": true,
    "id": "7lJPz6GhqZOl"
   },
   "source": [
    "Research how MCMC deals with multimodal distributions. For Gaussian mixture start in one mode and try to reach the other. Watch how this changes with increasing distance between the modes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "heading_collapsed": true,
    "hidden": true,
    "id": "_EwV5dK0qZOm"
   },
   "source": [
    "### Gaussian mixture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "rgfqf_hvqZOm"
   },
   "outputs": [],
   "source": [
    "run_many_proposals(\n",
    "    mixture,\n",
    "    proposals=[\n",
    "        RandomWalk(1.0), \n",
    "        Langevin(1.0, d=mixture)\n",
    "    ],\n",
    "    options = dict(\n",
    "        initial_point=torch.tensor([0., 0.]),\n",
    "        n_samples=10000,\n",
    "        n_parallel=10\n",
    "    ),\n",
    "    bounds=[(-4, 8), (-4, 8)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "2EJpQYbFqZOp"
   },
   "outputs": [],
   "source": [
    "dst = MixtureOfNormals(\n",
    "    locs=[torch.tensor([0., 0.]), torch.tensor([4., 4.])], \n",
    "    scales=[0.4, 0.4],\n",
    "    pi=torch.tensor([0.3, 0.7])\n",
    ")\n",
    "bnds = [(-2, 5), (-2, 5)]\n",
    "plot_distribution(dst, bnds, plt.gca(), n_levels=50)\n",
    "plot_distribution_grad(dst, bnds, plt.gca())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "3y6Lp6FPqZOr"
   },
   "outputs": [],
   "source": [
    "# Try changing the scales of mixture parts and observe how samples change\n",
    "\n",
    "dst = MixtureOfNormals(\n",
    "        locs=[torch.tensor([0., 0.]), torch.tensor([4., 4.])], \n",
    "        scales=[0.7, 0.7],\n",
    "        pi=torch.tensor([0.3, 0.7])\n",
    "    )\n",
    "\n",
    "run_many_proposals(\n",
    "    dst,\n",
    "    proposals=[\n",
    "        RandomWalk(0.3), \n",
    "        Langevin(0.1, d=dst),\n",
    "        HMC(0.1, d=dst)\n",
    "    ],\n",
    "    options = dict(\n",
    "        initial_point=torch.tensor([0., 0.]),\n",
    "        n_samples=3000,\n",
    "        n_parallel=10\n",
    "    ),\n",
    "    bounds=[(-2, 5), (-2, 5)]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "heading_collapsed": true,
    "hidden": true,
    "id": "lNGrQYHpqZOt"
   },
   "source": [
    "### Langevin mixture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "JNYiBXNSqZOu"
   },
   "outputs": [],
   "source": [
    "run_many_proposals(\n",
    "    langevin,\n",
    "    proposals=[\n",
    "        RandomWalk(0.1), \n",
    "        Langevin(0.01, d=langevin)\n",
    "    ],\n",
    "    options = dict(\n",
    "        initial_point=torch.tensor([0., 1.]),\n",
    "        n_samples=3000,\n",
    "        n_parallel=10\n",
    "    ),\n",
    "    bounds=[(-1, 2), (-3, 3)]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "heading_collapsed": true,
    "id": "B983CNjcqZOv"
   },
   "source": [
    "## Detection of convergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "hidden": true,
    "id": "lP4c9uGzqZOv"
   },
   "source": [
    "In previous experiments we usually started from the mode of the distribution. This is not the case in many situations and in fact we might not even start from a typical point from the distribution. We need means to detect MCMC convergence. The easiest way to detect it by watching statistics of $N$ last samples and/or same statistics of different chains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "ZBThM5xYqZOw"
   },
   "outputs": [],
   "source": [
    "def convergence_detection_experiment(distribution, proposals, options, bounds=[(-2, 2), (-3, 3)]):\n",
    "    D = options['initial_point'].shape[0]\n",
    "    fig = plt.figure(constrained_layout=True, figsize=(20, 10))\n",
    "    gs = fig.add_gridspec(2 * len(proposals), 1 + D)\n",
    "\n",
    "    for i, proposal in enumerate(proposals):\n",
    "        result = simulate(distribution, proposal, **options)\n",
    "        ax = [fig.add_subplot(gs[2*i:2*i+2, 0])] + [fig.add_subplot(gs[2*i, 1+k]) for k in range(D)] + [fig.add_subplot(gs[2*i+1, 1+k]) for k in range(D)]\n",
    "        plot_points(result['points'], ax[0])\n",
    "        plot_statistics(result['points'], ax[1:1 + D])\n",
    "        plot_traceplot(result['points'], ax[1 + D: 1 + 2 * D])\n",
    "\n",
    "        ax[0].set_title(f'{proposal}, rejection rate = {result[\"rejection_rate\"]:.2f}')\n",
    "        ax[0].set_xlim(*bounds[0])\n",
    "        ax[0].set_ylim(*bounds[1])\n",
    "        ax[0].set_aspect('equal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "WkkZ5YKpqZOx"
   },
   "outputs": [],
   "source": [
    "# Watch at plots of statistics and find the right amount of samples for MCMC to converge\n",
    "# Try out different proposals\n",
    "\n",
    "convergence_detection_experiment(\n",
    "    langevin,\n",
    "    proposals=[\n",
    "        RandomWalk(0.1), \n",
    "        Langevin(0.001, d=langevin)\n",
    "    ],\n",
    "    options = dict(\n",
    "        initial_point=torch.tensor([-1., -1.]),\n",
    "        n_samples=10000,\n",
    "        n_parallel=3\n",
    "    )\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "amsTrYsFpoAV"
   },
   "source": [
    "## Centred/Non-centred parameterization\n",
    "\n",
    "In many ways the parameterization of your model counts. Look at the following example (it is called Neal's funnel):\n",
    "\n",
    "$$\\begin{align*}\n",
    "   & z \\sim \\mathcal{N}(0, 3) \\\\\n",
    "   & x \\sim \\mathcal{N}(0, \\exp(z/2))\n",
    "\\end{align*}$$\n",
    "\n",
    "We can sample from this distribution over $(x, z)$ directly using MCMC. However this distribution is rather complex as you can see from figure below. Instead, we can reparameterize it like that:\n",
    "\n",
    "$$\\begin{align*}\n",
    "   & \\bar{z} \\sim \\mathcal{N}(0, 1) & z = \\sqrt{3}\\bar{z} \\\\\n",
    "   & \\bar{x} \\sim \\mathcal{N}(0, 1) & x = \\exp{(z/4)}\\bar{x} \n",
    "\\end{align*}$$\n",
    "\n",
    "The distribution over $(\\bar{z}, \\bar{x})$ is just a spherical normal which is very easy to sample from for MCMC methods. This is quite toy example, however parameterization of the model plays important role in efficiency of variational inference schemes.\n",
    "\n",
    "You can find more about this issue in details [here](https://arxiv.org/pdf/1906.03028.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qmk_Gd9erMiR"
   },
   "outputs": [],
   "source": [
    "class Funnel(Distribution):\n",
    "    def __init__(self):\n",
    "        self.d = dist.Normal(loc=0., scale=np.sqrt(3.))\n",
    "        \n",
    "    def log_density(self, y):\n",
    "        x, z = y[:, 0], y[:, 1]\n",
    "        logp_z = self.d.log_prob(z).sum(dim=-1)\n",
    "        logp_x = dist.Normal(loc=0., scale=z.mul(0.25).exp()).log_prob(x)\n",
    "        return logp_z + logp_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VLsajFP7qrYY"
   },
   "outputs": [],
   "source": [
    "centred = Funnel()\n",
    "noncentred = Normal(loc=0., scale=torch.tensor([1., 1.]))\n",
    "\n",
    "fig, axes = plt.subplots(ncols=2, figsize=(10, 10))\n",
    "r = plot_distribution(centred, bounds=((-5, 5), (-5, 5)), ax=axes[0], num=100, n_levels=20, filled=True)[1]\n",
    "axes[0].set_title(\"Neal's funnel\")\n",
    "axes[0].set_aspect('equal')\n",
    "fig.colorbar(r, ax=axes[0])\n",
    "\n",
    "r = plot_distribution(noncentred, bounds=((-5, 5), (-5, 5)), ax=axes[1], num=100, n_levels=20, filled=True)[1]\n",
    "axes[1].set_title('non-centred parameterization')\n",
    "axes[1].set_aspect('equal')\n",
    "fig.colorbar(r, ax=axes[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "heading_collapsed": true,
    "id": "G8qgrEgPqZO0"
   },
   "source": [
    "## Symplecticity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "hidden": true,
    "id": "AznBJiAIqZO0"
   },
   "source": [
    "For HMC to work we need to employ some kind of integrator. Usually we use Leapfrog because it is symplectic second-order method. Try different methods and watch how error in Hamiltonian increases with number of integration steps. **Hint:** use `HMC.energy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gOiI_RPo_mI8"
   },
   "outputs": [],
   "source": [
    "options = dict(\n",
    "    initial_point=torch.tensor([0., 0.]),\n",
    "    n_samples=1000,\n",
    "    n_parallel=5\n",
    ")\n",
    "\n",
    "base_eps = 0.05\n",
    "steps = [1, 5, 10, 50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "Y_4RILhhqZO1"
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(ncols=2, nrows=2, figsize=(10, 10))\n",
    "energies = []\n",
    "\n",
    "for i, n_steps in enumerate(steps):\n",
    "    proposal = HMC(eps=base_eps / n_steps, d=langevin, n_steps=n_steps, method='simple_euler')\n",
    "    ax = axes[i // 2][i % 2]\n",
    "    result = simulate(langevin, proposal, **options)\n",
    "    plot_points(result['points'], ax, color=False)\n",
    "    ax.set_title(f'n_steps={n_steps}, rejection rate = {result[\"rejection_rate\"]:.2f}')\n",
    "    en = np.stack([x.numpy() for x in proposal.energy], axis=0)\n",
    "    energies.append(en)\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "nXh5LMfwqZO4"
   },
   "outputs": [],
   "source": [
    "for en, n_steps in zip(energies, steps):\n",
    "  plt.plot(en - en[:1, :], label=f\"{n_steps}\")\n",
    "  print(np.sqrt(((en - en[:1, :])**2).sum(axis=0)).mean())"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "r8Hm6eohqZON",
    "JmBZEkmeqZOV",
    "tEql8eBrqZOa",
    "7jL6j-UPqZOg",
    "t9mPos2nqZOl",
    "B983CNjcqZOv"
   ],
   "name": "Markov Chain Monte-Carlo.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
